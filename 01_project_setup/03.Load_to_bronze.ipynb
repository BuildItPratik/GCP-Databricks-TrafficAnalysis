{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a665c0-5a34-4860-a77e-59e3c1935d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(name=\"env\",defaultValue='',label='Enter the environment in lower case')\n",
    "env = dbutils.widgets.get(\"env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "360729fa-450d-4f91-8632-1deb1fc6fa51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./commons\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e6571f1-04b3-40e4-89ef-899a95c761a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating a read_Traffic_Data() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd72395c-65ae-4437-a1ce-4152b11da7be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_Traffic_Data():\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "    print(\"Reading the Raw Traffic Data :  \", end='')\n",
    "    schema = StructType([\n",
    "        StructField(\"Record_ID\",IntegerType()),\n",
    "        StructField(\"Count_point_id\",IntegerType()),\n",
    "        StructField(\"Direction_of_travel\",StringType()),\n",
    "        StructField(\"Year\",IntegerType()),\n",
    "        StructField(\"Count_date\",StringType()),\n",
    "        StructField(\"hour\",IntegerType()),\n",
    "        StructField(\"Region_id\",IntegerType()),\n",
    "        StructField(\"Region_name\",StringType()),\n",
    "        StructField(\"Local_authority_name\",StringType()),\n",
    "        StructField(\"Road_name\",StringType()),\n",
    "        StructField(\"Road_Category_ID\",IntegerType()),\n",
    "        StructField(\"Start_junction_road_name\",StringType()),\n",
    "        StructField(\"End_junction_road_name\",StringType()),\n",
    "        StructField(\"Latitude\",DoubleType()),\n",
    "        StructField(\"Longitude\",DoubleType()),\n",
    "        StructField(\"Link_length_km\",DoubleType()),\n",
    "        StructField(\"Pedal_cycles\",IntegerType()),\n",
    "        StructField(\"Two_wheeled_motor_vehicles\",IntegerType()),\n",
    "        StructField(\"Cars_and_taxis\",IntegerType()),\n",
    "        StructField(\"Buses_and_coaches\",IntegerType()),\n",
    "        StructField(\"LGV_Type\",IntegerType()),\n",
    "        StructField(\"HGV_Type\",IntegerType()),\n",
    "        StructField(\"EV_Car\",IntegerType()),\n",
    "        StructField(\"EV_Bike\",IntegerType())\n",
    "        ])\n",
    "\n",
    "    rawTraffic_stream = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\",\"csv\")\n",
    "        .option('cloudFiles.schemaLocation',f'{checkpoint}/rawTrafficLoad/schemaInfer')\n",
    "        .option('header','true')\n",
    "        .schema(schema)\n",
    "        .load(landing+'/raw_traffic/')\n",
    "        .withColumn(\"Extract_Time\", current_timestamp())) # add timestamp to track the time\n",
    "    \n",
    "    print('Reading Succcess !!')\n",
    "    print('*******************')\n",
    "\n",
    "    return rawTraffic_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34af9275-c50a-4d1a-be97-7255c65ed3b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating read_Road_Data() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b865aab-436b-4f9b-8080-268ec98ad3fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_Road_Data():\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "    print(\"Reading the Raw Roads Data :  \", end='')\n",
    "    schema = StructType([\n",
    "        StructField('Road_ID',IntegerType()),\n",
    "        StructField('Road_Category_Id',IntegerType()),\n",
    "        StructField('Road_Category',StringType()),\n",
    "        StructField('Region_ID',IntegerType()),\n",
    "        StructField('Region_Name',StringType()),\n",
    "        StructField('Total_Link_Length_Km',DoubleType()),\n",
    "        StructField('Total_Link_Length_Miles',DoubleType()),\n",
    "        StructField('All_Motor_Vehicles',DoubleType())\n",
    "        \n",
    "        ])\n",
    "\n",
    "    rawRoads_stream = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\",\"csv\")\n",
    "        .option('cloudFiles.schemaLocation',f'{checkpoint}/rawRoadsLoad/schemaInfer')\n",
    "        .option('header','true')\n",
    "        .schema(schema)\n",
    "        .load(landing+'/raw_roads/')\n",
    "        )\n",
    "    \n",
    "    print('Reading Succcess !!')\n",
    "    print('*******************')\n",
    "\n",
    "    return rawRoads_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb3e75d-8789-4267-92cc-6b774529e634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating write_Road_Data(StreamingDF,environment) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5aa2a5-d602-4f70-9a85-db6accebde03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_Road_Data(StreamingDF,environment):\n",
    "    print(f'Writing data to {environment}_catalog raw_roads table', end='' )\n",
    "    write_Data = (StreamingDF.writeStream\n",
    "                    .format('delta')\n",
    "                    .option(\"checkpointLocation\",checkpoint + '/rawRoadsLoad/Checkpt')\n",
    "                    .outputMode('append')\n",
    "                    .queryName('rawRoadsWriteStream')\n",
    "                    .trigger(availableNow=True) # stops the streaming once data is written --> batch\n",
    "                    .toTable(f\"`{environment}_catalog`.`bronze`.`raw_roads`\"))\n",
    "    \n",
    "    write_Data.awaitTermination()\n",
    "    print('Write Success')\n",
    "    print(\"****************************\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cbd712e-e4fc-4548-9a3f-03e35d96d974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating write_Traffic_Data(StreamingDF,environment) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4aeda0-4ac9-4bba-aebd-637c6e57124e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_Traffic_Data(StreamingDF,environment):\n",
    "    print(f'Writing data to {environment}_catalog raw_traffic table', end='' )\n",
    "    write_Stream = (StreamingDF.writeStream\n",
    "                    .format('delta')\n",
    "                    .option(\"checkpointLocation\",checkpoint + '/rawTrafficLoad/Checkpt')\n",
    "                    .outputMode('append')\n",
    "                    .queryName('rawTrafficWriteStream')\n",
    "                    .trigger(availableNow=True) # stops the streaming once data is written --> batch\n",
    "                    .toTable(f\"`{environment}_catalog`.`bronze`.`raw_traffic`\"))\n",
    "    \n",
    "    write_Stream.awaitTermination()\n",
    "    print('Write Success')\n",
    "    print(\"****************************\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcfaceab-f235-4269-b93c-ea26648ef824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Calling read and Write Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a455f1-54b3-43da-bf55-1d3e83ab5bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Reading the raw_traffic's data from landing to Bronze\n",
    "read_Df = read_Traffic_Data()\n",
    "\n",
    "## Reading the raw_roads's data from landing to Bronze\n",
    "read_roads = read_Road_Data()\n",
    "\n",
    "## Writing the raw_traffic's data from landing to Bronze\n",
    "write_Traffic_Data(read_Df,env)\n",
    "\n",
    "## Writing the raw_roads's data from landing to Bronze\n",
    "write_Road_Data(read_roads,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5668620-fd0d-4b0c-82e7-7b08028f95c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT COUNT(*) FROM `{env}_catalog`.`bronze`.`raw_traffic` Limit 10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe81bf6c-a48e-42ca-b28f-a0426a2a8f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT COUNT(*) FROM `{env}_catalog`.`bronze`.`raw_roads` LIMIT 10\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03.Load_to_bronze",
   "widgets": {
    "env": {
     "currentValue": "dev",
     "nuid": "fd556224-67cf-420e-9974-44d8f7bc3cb8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
